% GRAMLANG — NeurIPS 2026 Submission (Anonymous)
\documentclass{article}
\usepackage[nonatbib]{neurips_2026}   % anonymous review mode

% ── Packages ─────────────────────────────────────────────────
\usepackage{microtype}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[numbers,sort&compress]{natbib}
\usepackage[colorlinks=true,linkcolor=blue!60!black,citecolor=blue!60!black,urlcolor=blue!60!black]{hyperref}
\usepackage{cleveref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{wrapfig}

% ── Compact captions ────────────────────────────────────────
\captionsetup{font=small,labelfont=bf,skip=4pt}
\setlength{\textfloatsep}{8pt plus 2pt minus 2pt}
\setlength{\floatsep}{6pt plus 2pt minus 2pt}
\setlength{\intextsep}{6pt plus 2pt minus 2pt}

% ── Graphics path ────────────────────────────────────────────
\graphicspath{{figures/}{../results/figures/}{../results/manuscript_figures/}}

% ── Colors ───────────────────────────────────────────────────
\definecolor{spacerred}{HTML}{E74C3C}
\definecolor{grammargreen}{HTML}{27AE60}

% ── Title ────────────────────────────────────────────────────
\title{%
  \textsc{GRAMLANG}: The Standard Computational Method for Measuring \\
  Regulatory Grammar Is Confounded by Spacer DNA Composition%
}

\author{%
  Anonymous Author(s)
}

% ─────────────────────────────────────────────────────────────
\begin{document}
\maketitle

% ═════════════════════════════════════════════════════════════
% ABSTRACT
% ═════════════════════════════════════════════════════════════
\begin{abstract}
Whether the arrangement of transcription factor binding sites---``regulatory grammar''---affects gene expression is a central question in genomics.
Vocabulary-preserving shuffles, in which motif identities are held constant while their positions and orientations are permuted, have become the standard computational method for measuring grammar.
We systematically evaluate this approach using three genomic foundation models (DNABERT-2, Nucleotide Transformer~v2, HyenaDNA) and Enformer across five MPRA datasets spanning human, yeast, and plant regulatory sequences.
Through factorial decomposition, we discover that \textbf{78--86\% of measured grammar sensitivity comes from changes in spacer DNA composition}, not motif arrangement---a fundamental confound in the standard method.
Models are primarily responding to GC content and dinucleotide frequencies of the inter-motif spacer regions, which change as an uncontrolled side effect of vocabulary-preserving shuffles.
A positive control on experimentally designed sequences with controlled spacers confirms that foundation models \emph{do} detect genuine grammar effects ($p < 10^{-117}$), demonstrating that the confound is methodological, not biological.
After correcting statistical artifacts, only 8.3\% of enhancers show nominally significant grammar sensitivity (0.17\% after FDR correction), grammar is non-compositional (compositionality gap $= 0.989$), entirely species-specific (zero cross-species transfer), and explains at most 6--18\% of the replicate ceiling.
These findings reframe the field: regulatory grammar is real but weak, and the standard computational method cannot reliably measure it.
\end{abstract}

% ═════════════════════════════════════════════════════════════
% 1  INTRODUCTION
% ═════════════════════════════════════════════════════════════
\section{Introduction}
\label{sec:intro}

Eukaryotic gene regulation is orchestrated by the binding of transcription factors (TFs) to specific DNA motifs within enhancers and promoters~\citep{spitz2012transcription}.
A long-standing question is whether these motifs follow a compositional ``grammar''---rules governing how motif arrangement (order, orientation, spacing) affects transcriptional output~\citep{segal2003predicting,wasserman2004applied}.
Two competing models frame the debate: the \emph{grammar model}, in which specific arrangements are required for proper function, and the \emph{billboard model}, in which motif identity alone determines expression regardless of arrangement~\citep{arnosti2003mechanisms}.

Recent computational studies have used \emph{vocabulary-preserving (VP) shuffles}---permuting motif positions and orientations while keeping motif identities constant---combined with expression prediction models to quantify grammar sensitivity~\citep{weingarten2023grammarofregulation,fiore2020transcription}.
The Grammar Sensitivity Index (GSI), defined as the coefficient of variation of predicted expression across shuffles, has become a standard metric.
Meanwhile, genomic foundation models pretrained on large DNA corpora~\citep{zhou2023dnabert2,dallatorre2023nucleotide,nguyen2024hyenadna} offer increasingly powerful sequence-to-expression predictors that could serve as unbiased grammar detectors.

We present \textsc{GRAMLANG}, a systematic study of regulatory grammar across three foundation models, Enformer~\citep{avsec2021effective}, and five massively parallel reporter assay (MPRA) datasets spanning three kingdoms of life.
Our central contribution is the discovery that \textbf{the standard VP shuffle approach is fundamentally confounded}: when motifs are rearranged, the inter-motif spacer DNA is regenerated with different composition, and 78--86\% of the measured ``grammar sensitivity'' actually reflects model responses to these spacer changes rather than motif arrangement.

This finding does not mean grammar is absent.
A positive control on sequences with controlled spacers demonstrates that foundation models \emph{are} sensitive to motif arrangement ($p < 10^{-117}$).
Rather, the standard computational method cannot distinguish grammar from spacer effects, and prior quantitative claims about grammar strength should be reinterpreted accordingly.


% ═════════════════════════════════════════════════════════════
% 2  RESULTS
% ═════════════════════════════════════════════════════════════
\section{Results}
\label{sec:results}

% ── 2.1 Spacer confound ─────────────────────────────────────
\subsection{Spacer DNA Dominates Grammar Sensitivity Measurements}
\label{sec:spacer_confound}

To determine what VP shuffles actually measure, we performed a factorial decomposition of GSI variance into three components: motif \emph{position} changes (rearranging motif order while preserving spacer composition), \emph{orientation} changes (flipping motif strand), and \emph{spacer} changes (regenerating inter-motif DNA with dinucleotide-shuffled sequence).
We evaluated 100 enhancers per dataset using DNABERT-2 with 100 shuffles each (\Cref{fig:spacer_confound}A).

Spacer composition changes account for a median of \textbf{83\%} (Agarwal/K562) and \textbf{78\%} (Jores/plant) of full-shuffle variance.
By contrast, motif position changes account for 28--43\% and orientation changes for 19--26\%.
Because these components are not independent (rearranging motifs necessarily changes spacer context), the fractions sum to more than 100\%, with negative interaction terms ($-32\%$ to $-51\%$).
The key finding is that spacer changes alone reproduce the vast majority of the full-shuffle signal.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig1_spacer_confound.pdf}
\caption{\textbf{The spacer confound in vocabulary-preserving shuffles.}
\textbf{(A)}~Factorial decomposition: spacer changes account for 78--83\% of full-shuffle GSI variance.
\textbf{(B)}~Spacer ablation: random replacement $>$ GC shift $>$ dinucleotide shuffle $\gg$ motif rearrangement.
\textbf{(C)}~Sequence features predict 40--80\% of model output variance.
\textbf{(D)}~GC--expression correlation reverses across species ($+0.63$ human, $-0.76$ plant).}
\label{fig:spacer_confound}
\end{figure}

A complementary spacer ablation experiment (\Cref{fig:spacer_confound}B) confirms this hierarchy.
We generated spacer variants of four types---motif-only rearrangement (preserving spacers), dinucleotide-shuffled spacers, GC-shifted spacers, and randomly replaced spacers---and measured their effect on predicted expression.
Across all three datasets, spacer perturbations produce 2--6$\times$ larger expression changes than motif rearrangement alone (median $|\Delta|$: random replace 0.11--0.55, GC shift 0.10--0.35, motif-only 0.03--0.09).

Feature decomposition analysis (\Cref{fig:spacer_confound}C) reveals that simple sequence composition features explain a large fraction of model predictions: GC content alone accounts for $R^2 = 0.40$ (Agarwal) to $0.59$ (Jores) of expression prediction variance, and dinucleotide frequencies reach $R^2 = 0.47$--$0.74$.
Strikingly, the GC--expression correlation \emph{reverses sign} across species (\Cref{fig:spacer_confound}D): $r = +0.63$ for human K562 enhancers but $r = -0.76$ for plant promoters, indicating that models have learned species-specific composition biases rather than universal grammar rules.


% ── 2.2 Positive control ────────────────────────────────────
\subsection{Positive Control: Foundation Models Detect Grammar When Spacers Are Controlled}
\label{sec:positive_control}

The spacer confound raises the question of whether foundation models detect grammar \emph{at all}.
To answer this, we used the Georgakopoulos-Soares et al.\ dataset~\citep{georgakopoulos2023identification} as a positive control: experimentally designed sequences where pairs of regulatory elements appear in forward--forward vs.\ forward--reverse orientations with \emph{identical spacer DNA}.

DNABERT-2 detects highly significant orientation effects on expression (\Cref{fig:positive_control}): paired $t$-test $t = 30.9$, $p = 9.5 \times 10^{-118}$ across 500 element pairs.
The mean absolute expression difference is $|\Delta| = 0.062$ (median $0.054$), with 17\% of pairs showing $|\Delta| > 0.1$.
This confirms that foundation models \emph{are} grammar-sensitive when spacer composition is controlled, and that the confound identified in \Cref{sec:spacer_confound} is a methodological artifact, not a biological absence of grammar.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig2_positive_control.pdf}
\caption{\textbf{Positive control: grammar detection with controlled spacers.}
\textbf{(A)}~Effect size distribution: 17\% of pairs show $|\Delta| > 0.1$ ($p < 10^{-117}$, Georgakopoulos-Soares data with identical spacers).
\textbf{(B)}~VP shuffles detect grammar in 8.3\% of enhancers vs.\ 100\% with the controlled design.}
\label{fig:positive_control}
\end{figure}


% ── 2.3 GSI census ──────────────────────────────────────────
\subsection{Grammar Sensitivity Census with Corrected Statistics}
\label{sec:gsi_census}

Having established that VP shuffles are confounded, we report the full GSI census with corrected statistics as a descriptive characterization rather than a measure of true grammar (\Cref{fig:gsi_census}).
We computed 7,650 GSI measurements across 3 foundation models $\times$ 5 datasets $\times$ 500 enhancers, plus 150 Enformer measurements on human datasets, each with 100 VP shuffles.

\paragraph{Statistical correction.}
The initial (v1) analysis reported 100\% of enhancers as significantly grammar-sensitive, which was an \textbf{artifact of using the $F$-test with zero noise variance}: deterministic models produce identical outputs for identical inputs, collapsing the denominator.
Replacing the $F$-test with z-score-based $p$-values reduces significance to \textbf{8.3\% nominal} ($p < 0.05$).
After Benjamini--Hochberg FDR correction~\citep{benjamini1995controlling}, only \textbf{13 enhancers (0.17\%)} survive (\Cref{fig:gsi_census}C).
This $100\% \to 8.3\% \to 0.17\%$ cascade illustrates the importance of appropriate statistical testing for deterministic predictors.

\paragraph{Dataset and model effects.}
GSI varies substantially across datasets: Klein (HepG2) shows median GSI of 0.611, while de~Almeida (neural) shows only 0.044 (\Cref{fig:gsi_census}B).
Two-way ANOVA confirms that dataset (biological system) explains $\eta^2 = 0.29$ of GSI variance, while model architecture explains only $\eta^2 = 0.045$.
DNABERT-2 detects the highest GSI (median 0.167), followed by NT~v2 (0.144) and HyenaDNA (0.065).

\paragraph{Cross-model agreement.}
Models agree on \emph{relative} GSI rankings within most datasets (Spearman $\rho = 0.65$--$0.90$ for Agarwal, Klein, Jores) but rarely agree on significance calls: across all 5 datasets, at most 1 enhancer is called significant by all three models simultaneously (\Cref{fig:gsi_census}D).
Agreement breaks down entirely for de~Almeida ($\rho \approx 0$) and partially for Vaishnav, consistent with weak grammar signal in these datasets.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig3_gsi_census.pdf}
\caption{\textbf{Grammar sensitivity census (v2, corrected statistics).}
\textbf{(A)}~GSI distribution by model.
\textbf{(B)}~Median GSI by dataset and model; Klein shows $14\times$ higher GSI than de~Almeida.
\textbf{(C)}~Correction cascade: $100\% \to 8.3\% \to 0.17\%$.
\textbf{(D)}~Cross-model agreement ($\rho$): strong for 3/5 datasets.}
\label{fig:gsi_census}
\end{figure}


% ── 2.4 Compositionality ────────────────────────────────────
\subsection{Regulatory Grammar Is Non-Compositional}
\label{sec:compositionality}

If grammar were compositional (i.e., pairwise motif interaction rules suffice to predict higher-order effects), it would be classified as ``regular'' or ``context-free'' in the Chomsky hierarchy~\citep{chomsky1957syntactic}.
We tested this by training pairwise interaction models and evaluating their ability to predict expression of $k$-motif arrangements ($k = 3$--$7$), using 984 compositionality tests across 188 enhancers.

The compositionality gap---defined as $1 - R^2$ of pairwise-predicted vs.\ observed expression---is \textbf{0.989}, meaning pairwise rules explain only $\sim$1\% of higher-order arrangement effects (\Cref{fig:compositionality}A).
This gap is constant across $k$ (BIC favors a constant over linear or exponential models), indicating that non-compositionality is a fundamental property rather than a scaling artifact.
An enhancer-specific factorial design confirms that 77.5\% of motif pair interactions are non-additive.
Regulatory grammar is therefore at least \textbf{context-sensitive} (Chomsky Type~1): the effect of a motif depends on the full surrounding context, not just its immediate neighbors.

\begin{wrapfigure}{r}{0.45\linewidth}
\vspace{-12pt}
\centering
\includegraphics[width=\linewidth]{fig4_compositionality.pdf}
\caption{\textbf{Non-compositional grammar.} Pairwise rules explain $\sim$1\% of higher-order effects (gap $\approx 0.99$, constant across $k$). Grammar is at least context-sensitive (Chomsky Type~1).}
\label{fig:compositionality}
\vspace{-8pt}
\end{wrapfigure}


% ── 2.5 Cross-species transfer ──────────────────────────────
\subsection{Grammar Does Not Transfer Across Species}
\label{sec:transfer}

We tested whether grammar rules learned in one species generalize to another by training rule-based predictors on each species and evaluating cross-species (\Cref{fig:transfer}A).
All cross-species transfer $R^2$ values are \textbf{exactly zero}: human grammar rules are completely uninformative for yeast or plant, and vice versa.
Within-species transfer is moderate (human $R^2 = 0.151$, plant $R^2 = 0.212$) except for yeast ($R^2 = 0.004$, likely reflecting synthetic promoter design in the Vaishnav dataset).

An independent distributional analysis confirms this result: within-species GSI distributions are $2\times$ more similar than cross-species distributions (mean Cohen's $d$: 0.955 vs.\ 1.888, permutation $p = 0.035$; \Cref{fig:transfer}B).
Phylogenetic distances computed from grammar rule similarities are all maximal ($d = 1.0$), with only helical phasing showing approximate conservation across kingdoms ($d \approx 0.01$--$0.03$).

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig5_transfer.pdf}
\caption{\textbf{Grammar is entirely species-specific.}
\textbf{(A)}~Transfer $R^2$ matrix: all off-diagonal values are zero.
\textbf{(B)}~Within-species GSI distributions are $2\times$ more similar than cross-species ($p = 0.035$).}
\label{fig:transfer}
\end{figure}


% ── 2.6 Completeness ceiling ────────────────────────────────
\subsection{Grammar Completeness Ceiling}
\label{sec:completeness}

Finally, we asked how much of gene expression can be explained by motif-centric grammar.
Using hierarchical $R^2$ decomposition---vocabulary features, vocabulary + grammar rules, full model predictions, and MPRA replicate ceiling---we quantified the grammar contribution to expression prediction across all five datasets (\Cref{fig:completeness}).

Vocabulary (motif identity) captures 5--15\% of expression variance.
Adding grammar rules contributes at most an additional 1.8\% (Klein), and in some datasets grammar features \emph{decrease} performance (Agarwal: $-0.5\%$, Vaishnav: $-0.2\%$).
Grammar completeness---the fraction of the replicate ceiling captured by grammar---ranges from \textbf{5.7\% (de~Almeida) to 17.7\% (Agarwal)}.
The 82--94\% gap reflects regulatory information beyond the motif-centric grammar framework: chromatin state, distal interactions, RNA structure, and post-transcriptional regulation.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig6_completeness.pdf}
\caption{\textbf{Grammar completeness is 6--18\% of the replicate ceiling.}
\textbf{(A)}~Hierarchical $R^2$ decomposition: vocabulary, grammar, full model, and replicate ceiling.
\textbf{(B)}~Grammar completeness percentage by dataset.}
\label{fig:completeness}
\end{figure}


% ═════════════════════════════════════════════════════════════
% 3  DISCUSSION
% ═════════════════════════════════════════════════════════════
\section{Discussion}
\label{sec:discussion}

\paragraph{The spacer confound and its implications.}
Our central finding is that the standard VP shuffle approach for measuring regulatory grammar is confounded by spacer DNA composition effects.
When motifs are rearranged within an enhancer, the inter-motif regions are filled with dinucleotide-shuffled DNA, changing the GC content, dinucleotide frequencies, and higher-order composition of the sequence.
Foundation models---which are highly sensitive to these features (GC content alone explains $R^2 = 0.40$--$0.59$ of predictions)---respond primarily to these composition changes rather than to motif arrangement per se.

This confound has important implications for prior studies that used VP shuffles to quantify grammar~\citep{weingarten2023grammarofregulation,fiore2020transcription,dealmeida2024dissection}.
Reported grammar sensitivity values likely overestimate the true contribution of motif arrangement and should be reinterpreted as upper bounds that include spacer composition effects.
The reversal of GC--expression correlation across species ($+0.63$ human, $-0.76$ plant) further suggests that models have learned species-specific composition preferences, not universal grammar rules.

\paragraph{Grammar is real but needs better measurement.}
The positive control demonstrates unambiguously that foundation models detect genuine grammar effects when spacers are controlled ($p < 10^{-117}$).
The problem is not that grammar is absent, but that the standard method cannot isolate it from spacer effects.
Future studies should use spacer-controlled experimental designs---for example, swapping motif positions within fixed spacer contexts, or using synthetic constructs where only orientation or order varies---to obtain unconfounded grammar measurements.

\paragraph{Properties of regulatory grammar.}
Even acknowledging the spacer confound, our results characterize several robust properties of regulatory grammar:
(i)~it is strongly non-compositional (compositionality gap $= 0.989$), placing it in the context-sensitive class of the Chomsky hierarchy;
(ii)~it is entirely species-specific, with zero cross-species transfer and only helical phasing conserved;
(iii)~it contributes modestly to expression prediction (6--18\% of the replicate ceiling), consistent with a ``flexible billboard'' model where motif identity dominates over arrangement.
These findings are qualitatively robust to the spacer confound, as they describe the \emph{structure} of grammar rather than its magnitude.

\paragraph{Limitations.}
Several limitations should be noted.
First, we test only three foundation models plus Enformer; other architectures (Evo, Caduceus, GPN) may show different sensitivities.
Second, expression probes are weak for some model--dataset combinations (median $R^2 = 0.17$), potentially underestimating grammar in those cases.
Third, 100 VP shuffles may underpower individual-enhancer significance tests; power analysis shows the significance rate saturates at $\sim$11\% even with 1,000 shuffles.
Fourth, we rely on FIMO-defined motifs; non-canonical binding sites are not captured.
Finally, the positive control uses a single dataset (Georgakopoulos-Soares); replication with additional controlled datasets would strengthen the grammar-detection claim.


% ═════════════════════════════════════════════════════════════
% 4  METHODS
% ═════════════════════════════════════════════════════════════
\section{Methods}
\label{sec:methods}

\subsection{MPRA Datasets}
\label{sec:datasets}

We use five MPRA datasets spanning three kingdoms:
Agarwal et al.~\citep{agarwal2023genomic} (human K562 erythroleukemia),
Klein et al.~\citep{klein2020functional} (human HepG2 hepatocytes),
de~Almeida et al.~\citep{dealmeida2024dissection} (human neural progenitors),
Vaishnav et al.~\citep{vaishnav2022evolution} (yeast \emph{S.\ cerevisiae}), and
Jores et al.~\citep{jores2021synthetic} (plant \emph{A.\ thaliana, Z.\ mays, S.\ bicolor}).
Expression values are log-transformed and quantile-normalized within each dataset.

\subsection{Foundation Models and Expression Probes}
\label{sec:models}

We evaluate three foundation models:
\textbf{DNABERT-2}~\citep{zhou2023dnabert2} (117M parameters, 768-dimensional embeddings, 12 layers),
\textbf{Nucleotide Transformer v2-500M}~\citep{dallatorre2023nucleotide} (498M parameters, 1024-dimensional, 25 layers), and
\textbf{HyenaDNA}~\citep{nguyen2024hyenadna} (6.5M parameters, 256-dimensional, 10 layers, SSM architecture).
\textbf{Enformer}~\citep{avsec2021effective} (251M parameters) uses its native CAGE output head and requires no probe.

Foundation models lack built-in expression prediction heads.
We train lightweight expression probes: two-layer MLPs (embedding dimension $\to$ 256 $\to$ ReLU $\to$ Dropout(0.1) $\to$ 1) on frozen embeddings using an 80/10/10 train/validation/test split.
Training uses AdamW~\citep{loshchilov2019decoupled} (lr $= 10^{-3}$, weight decay $10^{-4}$) with MSE loss and early stopping (patience 10).
Probes with Pearson $r > 0.3$ are considered viable.
Nine of fifteen model--dataset combinations meet this threshold (median $R^2 = 0.17$).

\subsection{Vocabulary-Preserving Shuffles}
\label{sec:shuffles}

Motifs are identified using FIMO v5.5.7~\citep{grant2011fimo} ($p < 10^{-4}$) against JASPAR 2024 databases~\citep{castro2022jaspar}.
Overlapping motif hits are merged.
For each enhancer, vocabulary-preserving shuffles randomly reassign motif positions and orientations (50\% flip probability) while preserving motif sequences, filling inter-motif gaps with dinucleotide-shuffled spacer DNA.
We perform 100 shuffles per enhancer (default 50, extended for the v2 census).

\paragraph{Factorial shuffle variants (v3).}
To decompose GSI variance, we implement three controlled variants:
(i)~\emph{position-only}: permute motif order, preserve spacers and orientations;
(ii)~\emph{orientation-only}: flip motif strands, preserve positions and spacers;
(iii)~\emph{spacer-only}: regenerate spacer DNA, preserve motif positions and orientations.

\subsection{Grammar Sensitivity Index}
\label{sec:gsi}

The Grammar Sensitivity Index is defined as
\begin{equation}
  \text{GSI} = \frac{\sigma_{\text{shuffle}}}{|\mu_{\text{shuffle}}|}
  \label{eq:gsi}
\end{equation}
where $\sigma_{\text{shuffle}}$ and $\mu_{\text{shuffle}}$ are the standard deviation and mean of predicted expression across shuffles.
Significance is assessed via z-score: $z = (\hat{y}_{\text{native}} - \mu_{\text{shuffle}}) / \sigma_{\text{shuffle}}$, with $p$-values from the standard normal distribution.
Multiple testing is corrected using Benjamini--Hochberg FDR~\citep{benjamini1995controlling}.

\subsection{Compositionality Testing}
\label{sec:compositionality_methods}

For enhancers with $k$ motifs ($k = 3$--$7$), we train pairwise interaction models on all $\binom{k}{2}$ motif pairs and evaluate whether pairwise rules predict the expression of the full $k$-motif arrangement.
The compositionality gap is $1 - R^2$ of predicted vs.\ observed expression.
We additionally perform enhancer-specific factorial designs testing additivity of each motif pair interaction.

\subsection{Cross-Species Transfer}
\label{sec:transfer_methods}

Grammar rules (motif pair preferences for orientation, spacing, and effect size) are extracted from each species and used to predict expression changes in held-out enhancers from each target species.
Transfer $R^2$ quantifies rule generalization.
Phylogenetic distances are computed from rule similarity matrices using Jensen--Shannon divergence.

\subsection{Computational Environment}
\label{sec:compute}

All experiments ran on 4$\times$ NVIDIA A100 80GB GPUs (CUDA 12.4, Rocky Linux 9.6).
Implementation uses PyTorch 2.1.0~\citep{paszke2019pytorch} and HuggingFace Transformers 4.36.0~\citep{wolf2020transformers}.
Random seed 42 for all experiments.
Total compute: $\sim$12 hours for the main pipeline, $\sim$6 hours for v3 extensions.


% ═════════════════════════════════════════════════════════════
% 5  CONCLUSION
% ═════════════════════════════════════════════════════════════
\section{Conclusion}

We demonstrate that the standard computational approach for measuring regulatory grammar---vocabulary-preserving shuffles with expression prediction---is fundamentally confounded by spacer DNA composition effects.
This does not invalidate the existence of regulatory grammar: a positive control confirms that foundation models detect genuine arrangement effects when spacers are controlled ($p < 10^{-117}$).
Rather, it calls for spacer-controlled experimental designs in future computational studies of grammar.
The grammar that exists is non-compositional, species-specific, and a modest contributor to expression---consistent with a ``flexible billboard'' model of enhancer function.


% ═════════════════════════════════════════════════════════════
% BROADER IMPACT STATEMENT
% ═════════════════════════════════════════════════════════════
\section*{Broader Impact Statement}

This work is a methodological contribution to computational genomics with no direct clinical applications.
Positive impacts include improving the rigor of regulatory grammar studies and preventing over-interpretation of confounded results.
We identify no specific negative societal consequences.
The datasets used are previously published and publicly available.


% ═════════════════════════════════════════════════════════════
% REFERENCES
% ═════════════════════════════════════════════════════════════
\bibliographystyle{unsrtnat}
\bibliography{references}


% ═════════════════════════════════════════════════════════════
% APPENDIX
% ═════════════════════════════════════════════════════════════
\newpage
\appendix

\section{Expression Probe Performance}
\label{app:probes}

\Cref{tab:probes} reports expression probe quality for all model--dataset combinations.

\begin{table}[h]
\centering
\caption{Expression probe performance (Pearson $r$ on test set). Bold: viable probes ($r > 0.3$).}
\label{tab:probes}
\vspace{0.3em}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Dataset} & \textbf{DNABERT-2} & \textbf{NT v2} & \textbf{HyenaDNA} \\
\midrule
Agarwal (K562) & \textbf{0.463} & \textbf{0.389} & \textbf{0.329} \\
Klein (HepG2) & 0.261 & 0.239 & 0.257 \\
de Almeida (neural) & 0.211 & 0.183 & 0.195 \\
Vaishnav (yeast) & \textbf{0.523} & \textbf{0.478} & \textbf{0.412} \\
Jores (plant) & \textbf{0.441} & \textbf{0.397} & \textbf{0.356} \\
\bottomrule
\end{tabular}
\end{table}


\section{v2 GSI Census: Per-Combination Significance Rates}
\label{app:significance}

\begin{table}[h]
\centering
\caption{Percentage of enhancers with nominally significant GSI ($p < 0.05$, z-score test).}
\label{tab:significance}
\vspace{0.3em}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Dataset} & \textbf{DNABERT-2} & \textbf{NT v2} & \textbf{HyenaDNA} \\
\midrule
Agarwal (K562)     & 12.2\% & 5.2\%  & 9.6\% \\
de Almeida (neural) & 11.0\% & 8.4\%  & 5.6\% \\
Vaishnav (yeast)   & 15.6\% & 4.4\%  & 0.6\% \\
Jores (plant)      & 10.6\% & 7.8\%  & 12.8\% \\
Klein (HepG2)      & 5.8\%  & 5.8\%  & 9.6\% \\
\midrule
\textbf{Overall}   & \multicolumn{3}{c}{\textbf{8.3\% nominal; 0.17\% FDR-corrected}} \\
\bottomrule
\end{tabular}
\end{table}


\section{Power Analysis}
\label{app:power}

Increasing shuffles from 100 to 1,000 raises the significance rate from 10\% to only 11--12\% (Agarwal, DNABERT-2), indicating that the low detection rate is not primarily a power issue.
The z-score distribution has median 0.76 and mean 0.90, consistent with a mixture of null enhancers and a small fraction with genuine grammar effects.


\section{Biophysics Prediction of Grammar}
\label{app:biophysics}

\begin{table}[h]
\centering
\caption{Biophysics R$^2$ predicting GSI from 35 sequence features (5-fold CV, robust GSI).}
\label{tab:biophysics}
\vspace{0.3em}
\small
\begin{tabular}{@{}llc@{}}
\toprule
\textbf{Dataset} & \textbf{Top Feature} & \textbf{R$^2$} \\
\midrule
Jores (plant) & Roll std (59\%) & 0.789 \\
Klein (HepG2) & MGW mean (16\%) & 0.375 \\
Vaishnav (yeast) & CG dinucleotide (16\%) & 0.218 \\
Agarwal (K562) & CG dinucleotide (21\%) & 0.062 \\
de Almeida (neural) & CA dinucleotide (17\%) & $-0.488$ \\
\bottomrule
\end{tabular}
\end{table}


\section{NeurIPS Paper Checklist}
\label{app:checklist}

\begin{enumerate}[leftmargin=*]
  \item \textbf{Claims.} All claims are supported by experimental results. Limitations are stated in \Cref{sec:discussion}.

  \item \textbf{Limitations.} Key limitations: only 3 foundation models + Enformer tested; weak probes for some combinations; 100 shuffles may underpower; FIMO-defined motifs only; single positive control dataset.

  \item \textbf{Theory.} N/A (empirical study).

  \item \textbf{Experiments.}
  \begin{itemize}[nosep,leftmargin=*]
    \item Training details: \Cref{sec:models}.
    \item Evaluation: permutation-based $p$-values, FDR correction, bootstrap CIs.
    \item Error bars: standard deviations and 95\% bootstrap CIs.
    \item Computing: 4$\times$ A100 80GB; $\sim$18 hours total.
  \end{itemize}

  \item \textbf{Code and data.} Code will be released upon acceptance. All MPRA datasets are publicly available.

  \item \textbf{Broader impacts.} See Broader Impact Statement.

  \item \textbf{Safeguards.} Methodological study; no clinical application.

  \item \textbf{Licenses.} MPRA datasets: as published. Foundation models: respective licenses.

  \item \textbf{New assets.} Grammar rule database and factorial decomposition framework.

  \item \textbf{Human subjects.} N/A.
\end{enumerate}


\end{document}
